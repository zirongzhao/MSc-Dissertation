# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K_ERPBiiOtH3kBKYUNsjv66CMcTV6kJt

##Char level

Hyper-parameters
"""

number_of_grams = 3 #equals to (number_of_grams-1)th order Markov Chain
alpha = 1e-3

"""### $\pi_0$"""

x_train_regions['pi_0_char'] = x_train_regions['tweet_modified_with_end'].str[:number_of_grams-1]
x_train_regions.head()

pi_0_char_freq = pd.crosstab(x_train_regions['geo_tag'], x_train_regions['pi_0_char'])
pi_0_char_freq

pi_zero_dictionary_character_freq = pi_0_char_freq.unstack().to_dict()
#pi_zero_dictionary_character_freq

pi_0_char_prop = pd.crosstab(x_train_regions['geo_tag'], x_train_regions['pi_0_char'], normalize='index')
pi_0_char_prop

pi_zero_dictionary_character_prop = pi_0_char_prop.unstack().to_dict()
#pi_zero_dictionary_character

dict(itertools.islice(pi_zero_dictionary_character_prop.items(), 20))

dict(itertools.islice(pi_zero_dictionary_character_prop.items(), 10))

"""### Addivtive smoothing
Avoid log0 --> Perform addictive smoothing on the above table (i.e. minimum transition probability)
"""

for key, value in pi_zero_dictionary_character_prop.items():
  if value == 0:
    pi_zero_dictionary_character_prop[key] = alpha

for key, value in pi_zero_dictionary_character_prop.items():
  if key[1] == 'GB':
    print(key, pi_zero_dictionary_character_prop[key])

#Since we can expect that zero probability will be rare in the character model, results after smoothing do not change much  
dict(itertools.islice(pi_zero_dictionary_character_prop.items(), 20))

dict(itertools.islice(pi_zero_dictionary_character_prop.items(), 10))

def log_likelihood_pi_zero_char(tweet,number_of_grams):

  geo_region_list = ['GB','IN','US-California','US-New York']
  
  #Given a tweet, compute the log_likelihood_pi_zero of this tweet for different countries  

  log_likelihood_pi_zero_character = {}

  initial_character = ''.join(list(tweet)[:number_of_grams-1])
  
  #Compute the log_likelihood of pi_0
   #First check whether the fisrt character in a test tweet also exists in the training corpus
  if any(k[0] == initial_character for k in pi_zero_dictionary_character_prop.keys()):
    for i in geo_region_list: 
      log_likelihood_pi_zero_character[i] = np.log(pi_zero_dictionary_character_prop[(initial_character,i)])
  else:
    for i in geo_region_list:
      log_likelihood_pi_zero_character[i] = None
  
  return log_likelihood_pi_zero_character

#create a simple example to check accuracy
e1 = 'aaaa like uk' #start with a seen character
e2 = '@ like us' #start with an unseen character
print(log_likelihood_pi_zero_char(e1, number_of_grams))
print(log_likelihood_pi_zero_char(e2, number_of_grams))

"""### $N_{ij}$ & $P_{ij}$

Generate transition matrix for each tweet (entries are frequency and entries are prbablilties)
"""

#n_ij
from collections import defaultdict
def generate_transition_dicitionary_character_freq(tweet,number_of_grams):
  vocab = set(list(tweet))
  char_index = {char: i for i, char in enumerate(vocab)}
    
  # Create bigrams from all characters in a tweet
  n_grams = ngrams(list(tweet),number_of_grams)
    
  # Frequency distribution of bigrams ((char1, char2), num_occurrences)
  ngram_freq = Counter(n_grams)
  
  tp = {ngram: float(ngram_freq[ngram]) for ngram in ngram_freq}

  return tp

"""Concatenate tweets from the same geo-regions in the traning set"""

x_train_regions['tweet_modified_with_end'] = x_train_regions['tweet_modified_with_end'].astype(str)

x_train_concat = x_train_regions.groupby(['geo_tag'])['tweet_modified_with_end'].apply(lambda x: ''.join(x)).reset_index()

x_train_concat.columns.values

#N_ij for each class
geo_region_list = ['GB','IN','US-California','US-New York']
dictionary_tm_countries_char_freq = {}
for i in geo_region_list:
  index = x_train_concat.index[x_train_concat['geo_tag']==i]
  for idx,value in x_train_concat.iterrows():
    if idx == index:
      tweet_concat = value['tweet_modified_with_end']

  dictionary_tm_countries_char_freq[i] = generate_transition_dicitionary_character_freq(tweet_concat,number_of_grams)

dictionary_tm_countries_char_freq.keys()

dict(itertools.islice(dictionary_tm_countries_char_freq['GB'].items(), 10))

#p_ij
from collections import defaultdict
def generate_transition_dicitionary_character_prop(tweet,number_of_grams):
  vocab = set(list(tweet))
  char_index = {char: i for i, char in enumerate(vocab)}
    
  # Create bigrams from all characters in a tweet
  n_grams = ngrams(list(tweet),number_of_grams)
    
  # Frequency distribution of bigrams ((char1, char2), num_occurrences)
  ngram_freq = Counter(n_grams)

  # Compute the transition probability
  sum_same_initial_state = defaultdict(int)
  for t in ngram_freq:
    sum_same_initial_state[t[:number_of_grams-1]] += ngram_freq[t]
  
  tp = {ngram: float(ngram_freq[ngram])/sum_same_initial_state[ngram[:number_of_grams-1]] for ngram in ngram_freq}

  return tp

#p_ij for each class
geo_region_list = ['GB','IN','US-California','US-New York']
dictionary_tm_countries_char_prop = {}
for i in geo_region_list:
  index = x_train_concat.index[x_train_concat['geo_tag']==i]
  for idx,value in x_train_concat.iterrows():
    if idx == index:
      tweet_concat = value['tweet_modified_with_end']

  dictionary_tm_countries_char_prop[i] = generate_transition_dicitionary_character_prop(tweet_concat,number_of_grams)

dictionary_tm_countries_char_prop.keys()

for i in geo_region_list:
  dictionary_tm_countries_char_prop[i] = {key:value for key, value in dictionary_tm_countries_char_prop[i].items() if key[0] != '$'}

dict(itertools.islice(dictionary_tm_countries_char_prop['GB'].items(), 10))

"""### Addictive smoothing
Avoid log0 --> Perform addictive smoothing on the above table
"""

for i in geo_region_list:
  for key, value in dictionary_tm_countries_char_prop[i].items():
    if value == 0:
      dictionary_tm_countries_char_prop[i][key] = alpha

dictionary_tm_countries_char_prop['GB']

#Since we can expect that zero probability will be rare in the character model, results after smoothing do not change much
dict(itertools.islice(dictionary_tm_countries_char_prop['GB'].items(), 20))

"""### $\sum_{i,j}N_{i,j} \log P_{i,j}$"""

def log_likelihood_p_char(tweet, number_of_grams):

  log_likelihood_p_char = {}

  #Generate a dictionary for the frequency matrix (N_ij) of a test tweet 
  dictionary_test = generate_transition_dicitionary_character_freq(tweet,number_of_grams)

  #For each contry:
  geo_region_list = ['GB','IN','US-California','US-New York']
  for i in geo_region_list:

    #Extract the dictionary containing the transition matrix (P_ij) <- estimating from the training set
    dictionary_train = dictionary_tm_countries_char_prop[i]

    #Compute sum[N_ij * log(P_ij)]
    log_likelihood_list = []

     #Look for the transition states that both appear in the new tweet and the existing training set 
    dictionary_intersection_keys = set(dictionary_train.keys()) & set(dictionary_test.keys())
    dictionary_intersection = {k:dictionary_train[k] for k in (dictionary_intersection_keys) if k in dictionary_train}

    if len(dictionary_intersection_keys) == 0:
      log_likelihood_p_char[i] = None
    else:
      for key, value in dictionary_intersection.items():
        log_likelihood_element = dictionary_test[key] * np.log(dictionary_intersection[key])
        log_likelihood_list.append(log_likelihood_element)
       
        log_likelihood_p_char[i] = np.sum(log_likelihood_list)
  
  return log_likelihood_p_char

"""Examples"""

#Some examples to ensure that the calculation is correct
e1 = 'uk is a beauiful country'   #everything is fine
e2 = '@ is a beauiful country'   #no initial probability
e3 = 'u@@##'              #no transition probability
e4 = '@#@!#%^&*()'           #full of unseen words

part_1 = pd.Series(log_likelihood_pi_zero_char(e4,number_of_grams))
part_2 = pd.Series(log_likelihood_p_char(e4,number_of_grams))
print(part_1)
print(part_2)

"""Training performance"""

pred = []
count_zero_likelihood = 0

for tweet in x_train_regions['tweet_modified_with_end_selected']:
  part_1 = pd.Series(log_likelihood_pi_zero_char(tweet, number_of_grams, alpha))
  part_2 = pd.Series(log_likelihood_p_char(tweet, number_of_grams, alpha))
  if (part_1.isnull().all()) and (part_2.isnull().all()):
    geo_region_list = ['GB','IN','US-California','US-New York']
    max_country = np.random.choice(geo_region_list, p=[0.28, 0.25, 0.25, 0.22]) 
    count_zero_likelihood += 1
  else: 
    part_1_remove_none = part_1.fillna(0)
    part_2_remove_none = part_2.fillna(0)
    log_likelihood_sum = part_1_remove_none.add(part_2_remove_none)
    max_country = pd.to_numeric(log_likelihood_sum).idxmax()
  pred.append(max_country)

from collections import Counter
Counter(pred)

count_zero_likelihood

len(pred)

print(metrics.classification_report(y_train_geo, pred))

"""Validation Performance"""

pred = []
count_zero_likelihood = 0

for tweet in x_val_regions['tweet_modified_with_end']:
  
  #part_1 = 0 #(for zero order markov chain)
  part_1 = pd.Series(log_likelihood_pi_zero_char(tweet, number_of_grams))
  part_2 = pd.Series(log_likelihood_p_char(tweet, number_of_grams))
  
  #if part_2.isnull().all(): #(for zero order markov chain)
  if (part_1.isnull().all()) and (part_2.isnull().all()):
    geo_region_list = ['GB','IN','US-California','US-New York']
    max_country = np.random.choice(geo_region_list, p=[0.28, 0.24, 0.24, 0.24]) 
    count_zero_likelihood += 1
  else: 
    part_1_remove_none = part_1.fillna(0)
    part_2_remove_none = part_2.fillna(0)
    log_likelihood_sum = part_1_remove_none.add(part_2_remove_none)
    max_country = pd.to_numeric(log_likelihood_sum).idxmax()
  pred.append(max_country)

from collections import Counter
Counter(pred)

count_zero_likelihood

len(pred)

print(metrics.classification_report(y_validate_geo, pred, digits=4))







"""##Word level

i.e. split each tweet by word

Check vocabulary size
"""

training_vocabulary_word = set(x_train_regions['tweet_modified_with_end'].str.cat(sep=' ').split())
len(training_vocabulary_word)

val_vocabulary_word = set(x_val_regions['tweet_modified_with_end'].str.cat(sep=' ').split())
len(val_vocabulary_word)

unseen_word_size = len(val_vocabulary_word) - len(training_vocabulary_word.intersection(val_vocabulary_word))
unseen_word_size

unseen_word_size/len(val_vocabulary_word)

"""Hyper-parameters"""

number_of_grams = 1  #equals to (number_of_grams-1)th order Markov Chain
alpha = 1e-3

"""### $\pi_0$"""

x_train_regions['pi_0_word'] = x_train_regions['tweet_modified_with_end'].str.split().str.slice(start=0, stop=number_of_grams-1).apply(lambda x: ' '.join(x))
x_train_regions.head()

pi_0_word_freq = pd.crosstab(x_train_regions['geo_tag'], x_train_regions['pi_0_word'])
pi_0_word_freq

pi_zero_dictionary_word_freq = pi_0_word_freq.unstack().to_dict()
#pi_zero_dictionary_word_freq

pi_0_word_prob = pd.crosstab(x_train_regions['geo_tag'], x_train_regions['pi_0_word'], normalize='index')
pi_0_word_prob

pi_zero_dictionary_word_prop = pi_0_word_prob.unstack().to_dict()
#pi_zero_dictionary_word_prop

dict(itertools.islice(pi_zero_dictionary_word_prop.items(), 10))

"""### Addictive smoothing
Avoid log0 --> Perform addictive smoothing on the above table
"""

for key, value in pi_zero_dictionary_word_prop.items():
  if value == 0:
    pi_zero_dictionary_word_prop[key] = alpha

dict(itertools.islice(pi_zero_dictionary_word_prop.items(), 10))

def log_likelihood_pi_zero_word(tweet, number_of_grams):

  geo_region_list = ['GB','IN','US-California','US-New York']
  
  #Given a test example (tweet), compute the log_likelihood_pi_zero of this tweet for different countries  

  log_likelihood_pi_zero_word = {}

  initial_word = ' '.join(tweet.split()[:number_of_grams-1])

  #Compute the log_likelihood of pi_0
   #First check whether the fisrt word exists in the training corpus
  if any(k[0] == initial_word for k in pi_zero_dictionary_word_prop.keys()):
    for i in geo_region_list: 
      log_likelihood_pi_zero_word[i] = np.log(pi_zero_dictionary_word_prop[(initial_word,i)])
  else:
    for i in geo_region_list:
      log_likelihood_pi_zero_word[i] = None
  
  return log_likelihood_pi_zero_word

#Construct two example to check accuracy of this function
e1 = x_train_regions['tweet_modified_with_end'].iloc[100] #a tweet from the training set --> the starting word must exist
e2 = 'zirong loves sunny days.' #a tweet such that the starting word must not exist in the training corpus

print(log_likelihood_pi_zero_word(e1, number_of_grams))
print(log_likelihood_pi_zero_word(e2,number_of_grams))

"""### $N_{ij}$ & $P_{ij}$

Generate transition matrix for each tweet (entries are frequency and entries are prbablilties)
"""

#n_ij
from collections import defaultdict
def generate_transition_dicitionary_word_freq(tweet,number_of_grams):
  vocab = set(tweet.split())
  vocab_index = {word: i for i, word in enumerate(vocab)}
    
  # Create bigrams from all characters in a tweet
  n_grams = ngrams(tweet.split(),number_of_grams)
    
  # Frequency distribution of bigrams ((char1, char2), num_occurrences)
  ngram_freq = Counter(n_grams)
  
  tp = {ngram: float(ngram_freq[ngram]) for ngram in ngram_freq}

  return tp

"""Concatenate tweets from the same geo-regions in the traning set"""

x_train_regions['tweet_modified_with_end'] = x_train_regions['tweet_modified_with_end'].astype(str)

x_train_concat = x_train_regions.groupby(['geo_tag'])['tweet_modified_with_end'].apply(lambda x: ''.join(x)).reset_index()

x_train_concat.columns.values

#N_ij for each class
geo_region_list = ['GB','IN','US-California','US-New York']
dictionary_tm_countries_word_freq = {}
for i in geo_region_list:
  index = x_train_concat.index[x_train_concat['geo_tag']==i]
  for idx,value in x_train_concat.iterrows():
    if idx == index:
      tweet_concat = value['tweet_modified_with_end']

  dictionary_tm_countries_word_freq[i] = generate_transition_dicitionary_word_freq(tweet_concat,number_of_grams)

dictionary_tm_countries_word_freq.keys()

dict(itertools.islice(dictionary_tm_countries_word_freq['GB'].items(), 20))

#p_ij
from collections import defaultdict

def generate_transition_dicitionary_character_prop(tweet,number_of_grams):
  vocab = set(tweet.split())
  vocab_index = {word: i for i, word in enumerate(vocab)}
    
  # Create bigrams from all characters in a tweet
  n_grams = ngrams(tweet.split(),number_of_grams)
    
  # Frequency distribution of bigrams ((char1, char2), num_occurrences)
  ngram_freq = dict(Counter(n_grams))

  # Compute the transition probability
  sum_same_initial_state = defaultdict(int)
  for t in ngram_freq:
    sum_same_initial_state[t[:number_of_grams-1]] += ngram_freq[t]
  
  tp = {ngram: float(ngram_freq[ngram])/sum_same_initial_state[ngram[:number_of_grams-1]] for ngram in ngram_freq}

  return tp

#p_ij for each class
geo_region_list = ['GB','IN','US-California','US-New York']
dictionary_tm_countries_word_prop = {}
for i in geo_region_list:
  index = x_train_concat.index[x_train_concat['geo_tag']==i]
  for idx,value in x_train_concat.iterrows():
    if idx == index:
      tweet_concat = value['tweet_modified_with_end']

  dictionary_tm_countries_word_prop[i] = generate_transition_dicitionary_character_prop(tweet_concat,number_of_grams)

dictionary_tm_countries_word_prop.keys()

for i in geo_region_list:
  dictionary_tm_countries_word_prop[i] = {key:value for key, value in dictionary_tm_countries_word_prop[i].items() if key[0] != '$'}

dict(itertools.islice(dictionary_tm_countries_word_prop['GB'].items(), 20))

#set(dictionary_tm_countries_word['GB'].keys()) & set(dictionary_tm_countries_word['IN'].keys() & set(dictionary_tm_countries_word['US-California'].keys()) & set(dictionary_tm_countries_word['US-New York'].keys()) )

"""### Addictive smoothing
Avoid log0 --> Perform addictive smoothing on the above table
"""

for i in geo_region_list:
  for key, value in dictionary_tm_countries_word_prop[i].items():
    if value == 0:
      dictionary_tm_countries_word_prop[i][key] = alpha

dict(itertools.islice(dictionary_tm_countries_word_prop['GB'].items(), 20))

"""### $\sum_{i,j}N_{i,j} \log P_{i,j}$"""

def log_likelihood_p_word(tweet, number_of_grams):

  log_likelihood_p_word = {}

  #Generate a dictionary for the frequency matrix (N_ij) of a test tweet 
  dictionary_test = generate_transition_dicitionary_word_freq(tweet, number_of_grams)

  #For each contry:
  geo_region_list = ['GB','IN','US-California','US-New York']
  for i in geo_region_list:

    #Extract the dictionary containing the transition matrix (P_ij) <- estimating from the training set
    dictionary_train = dictionary_tm_countries_word_prop[i]

    #Compute sum[N_ij * log(P_ij)]
    log_likelihood_list = []

    #Look for the transition states that both appear in the test set and the training set 
    dictionary_intersection_keys = set(dictionary_train.keys()) & set(dictionary_test.keys())
    dictionary_intersection = {k:dictionary_train[k] for k in (dictionary_intersection_keys) if k in dictionary_train}

    if len(dictionary_intersection_keys) == 0:
      log_likelihood_p_word[i] = None
    
    else:
      for key, value in dictionary_intersection.items():
        log_likelihood_element = dictionary_test[key] * np.log(dictionary_intersection[key])
        log_likelihood_list.append(log_likelihood_element)
       
        log_likelihood_p_word[i] = np.sum(log_likelihood_list)
  
  return log_likelihood_p_word

"""Examples"""

#Some examples to ensure that the above calculation is correct
#Note that these examples are only valid for 2-grams. To test other number of grams, you should modify these examples

e1 = 'able to fighting covid stay safe safe stay' #everything is fine
e2 = 'zirong covid stay safe safe stay' #no initial probability
e3 = 'fighting @@##' #no transition probability
e4 = 'zirong @#$'#full of unseen words

part_1 = pd.Series(log_likelihood_pi_zero_word(e1, number_of_grams))
part_2 = pd.Series(log_likelihood_p_word(e1, number_of_grams))
print(part_1)
print(part_2)

"""Training performance"""

pred = []
count_zero_likelihood = 0

for tweet in x_train_regions['tweet_modified_with_end_selected']:
  part_1 = pd.Series(log_likelihood_pi_zero_word(tweet, number_of_grams))
  part_2 = pd.Series(log_likelihood_p_word(tweet, number_of_grams))
  if (part_1.isnull().all()) and (part_2.isnull().all()):
    geo_region_list = ['GB','IN','US-California','US-New York']
    max_country = np.random.choice(geo_region_list, p=[0.28, 0.25, 0.25, 0.22]) 
    count_zero_likelihood += 1
  else: 
    part_1_remove_none = part_1.fillna(0)
    part_2_remove_none = part_2.fillna(0)
    log_likelihood_sum = part_1_remove_none.add(part_2_remove_none)
    max_country = pd.to_numeric(log_likelihood_sum).idxmax()
  pred.append(max_country)

from collections import Counter
Counter(pred)

count_zero_likelihood

len(pred)

print(metrics.classification_report(y_train_geo, pred))

"""Validation Performance"""

pred = []
count_zero_likelihood = 0

for tweet in x_val_regions['tweet_modified_with_end']:
  
  part_1 = 0 #(for zero order markov chain)
  #part_1 = pd.Series(log_likelihood_pi_zero_word(tweet, number_of_grams))
  part_2 = pd.Series(log_likelihood_p_word(tweet, number_of_grams))
  
  if part_2.isnull().all(): #(for zero order markov chain)
  #if (part_1.isnull().all()) and (part_2.isnull().all()):
    geo_region_list = ['GB','IN','US-California','US-New York']
    max_country = np.random.choice(geo_region_list, p=[0.28, 0.24, 0.24, 0.24]) 
    count_zero_likelihood += 1
  else: 
    #part_1_remove_none = part_1.fillna(0) #(for zero order markov chain)
    #part_1_remove_none = part_1.fillna(0)
    part_2_remove_none = part_2.fillna(0)
    log_likelihood_sum = part_1_remove_none.add(part_2_remove_none)
    max_country = pd.to_numeric(log_likelihood_sum).idxmax()
  pred.append(max_country)

from collections import Counter
Counter(pred)

count_zero_likelihood

len(pred)

print(metrics.classification_report(y_validate_geo, pred, digits=4))